# EXAM STUDY GUIDE - Breast Cancer Prediction System
## For Quick Cramming - All Essential Concepts

**Author:** Ogah Victor (22CG031902)  
**Date:** January 2026

---

## üéØ QUICK FACTS (Memorize This!)

```
Algorithm: Logistic Regression
Accuracy: ~96-97%
Features Used: 5 (radius_mean, texture_mean, perimeter_mean, area_mean, smoothness_mean)
Dataset: Breast Cancer Wisconsin (569 samples)
Train-Test Split: 80-20
Scaling: StandardScaler (mandatory)
Model Persistence: Joblib (.pkl)
Framework: Streamlit
Deployment: Render
```

---

## üìö PART A: MODEL DEVELOPMENT (What You MUST Know)

### 1. LOGISTIC REGRESSION BASICS
```
Used For: Binary classification (Yes/No, Malignant/Benign)
Output: Probability between 0 and 1
Formula: y = 1 / (1 + e^(-x))  [Sigmoid function]
Advantage: Simple, interpretable, fast
Why It Works: Best for linearly separable data
```

### 2. FEATURE SCALING (CRITICAL!)
```
Why Important: Brings all features to same scale
Formula: (x - mean) / standard_deviation
What It Does: Centers data at 0, scales to unit variance
Tool Used: StandardScaler from sklearn
When Not Done: Model performs poorly or fails
```

### 3. TRAIN-TEST SPLIT
```
Purpose: Test model on unseen data
Ratio: 80% training, 20% testing (standard)
Stratify: YES (maintain class distribution)
Why: Prevents overfitting, ensures fair evaluation
```

### 4. DATA PREPROCESSING STEPS (In Order!)
```
Step 1: Load data (sklearn.datasets.load_breast_cancer)
Step 2: Select features (5 features)
Step 3: Check missing values (usually none in this dataset)
Step 4: Split data (train_test_split)
Step 5: Scale features (StandardScaler.fit_transform on training data)
Step 6: Train model (fit on training data)
Step 7: Evaluate (test on test data)
Step 8: Save model (joblib.dump)
```

### 5. 5 SELECTED FEATURES (Memorize!)
```
1. radius_mean - Average radius of tumor
2. texture_mean - Texture/gray-scale properties
3. perimeter_mean - Boundary size of tumor
4. area_mean - Total area of tumor
5. smoothness_mean - Surface smoothness variation
```

---

## üìä PART B: EVALUATION METRICS (Must Know!)

### ACCURACY
```
Definition: Overall correctness of predictions
Formula: (True Positives + True Negatives) / Total Predictions
Range: 0 to 1 (or 0-100%)
Example: 96.5% = 96.5 out of 100 predictions correct
When to Use: Balanced dataset, overall performance
```

### PRECISION
```
Definition: Of predicted Malignant, how many were actually Malignant?
Formula: True Positives / (True Positives + False Positives)
Range: 0 to 1
Why Care: Avoid false alarms (wrong diagnosis)
Example: 96% precision = 96% of "malignant" predictions are correct
```

### RECALL (SENSITIVITY)
```
Definition: Of actual Malignant cases, how many did we find?
Formula: True Positives / (True Positives + False Negatives)
Range: 0 to 1
Why Care: Don't miss real malignant cases
Example: 97% recall = Found 97% of actual malignant cases
Critical: In medical context, missing cancer is worse than false alarm
```

### F1-SCORE
```
Definition: Balanced score between Precision and Recall
Formula: 2 √ó (Precision √ó Recall) / (Precision + Recall)
Range: 0 to 1
Use When: Need both precision and recall to be good
Medical Context: Best overall metric for this problem
```

### CONFUSION MATRIX
```
                Predicted Negative  |  Predicted Positive
Actual Negative    True Negative     |  False Positive
Actual Positive    False Negative    |  True Positive

Interpretation:
- Diagonal (TN, TP): Correct predictions
- Off-diagonal (FN, FP): Wrong predictions
- FN worst in medical context (missing cancer)
```

---

## üíæ PART C: MODEL PERSISTENCE (Save & Load)

### JOBLIB FORMAT
```python
# SAVING
import joblib
joblib.dump(model, 'breast_cancer_model.pkl')
joblib.dump(scaler, 'scaler.pkl')

# LOADING
model = joblib.load('breast_cancer_model.pkl')
scaler = joblib.load('scaler.pkl')

# ADVANTAGES
‚úì Fast serialization
‚úì Handles NumPy arrays
‚úì Preserves all model parameters
‚úì Can reload without retraining
```

### PREDICTION WITH SAVED MODEL
```python
# New input data
features = [[12, 18, 80, 500, 0.10]]

# Scale features (IMPORTANT!)
features_scaled = scaler.transform(features)

# Get prediction
prediction = model.predict(features_scaled)  # Output: 0 or 1
probability = model.predict_proba(features_scaled)  # [P(0), P(1)]

# Interpretation
# 0 = Malignant, 1 = Benign
# Probability = confidence of prediction
```

---

## üåê PART D: STREAMLIT APP BASICS

### WHAT IT DOES
```
- Loads saved model and scaler from disk
- Gets user input via interactive sliders
- Scales input features
- Makes prediction
- Displays results with confidence
```

### KEY STREAMLIT FUNCTIONS (You Should Know!)
```python
st.slider()          # For numeric input (radius, texture, etc.)
st.button()          # Trigger predictions
st.metric()          # Display key metrics
st.warning()         # Show important messages
st.markdown()        # Format text/HTML
st.table()           # Display dataframes
st.cache_resource()  # Cache model loading for speed
```

### CODE STRUCTURE
```python
# 1. Page configuration
st.set_page_config(...)

# 2. Load model (cached)
@st.cache_resource
def load_model():
    model = joblib.load('model/breast_cancer_model.pkl')
    scaler = joblib.load('model/scaler.pkl')
    return model, scaler

# 3. Get inputs from user
# (Sliders, buttons, etc.)

# 4. Make prediction
prediction = model.predict(scaler.transform(input))

# 5. Display results
st.write(f"Diagnosis: {prediction}")
st.metric("Confidence", f"{probability*100:.2f}%")
```

---

## üöÄ PART E: DEPLOYMENT ON RENDER (Steps!)

### GITHUB SETUP (MUST DO FIRST!)
```
1. Create GitHub repository
2. Push all code to GitHub
3. Make sure it's PUBLIC
4. Keep model files in /model/ directory
```

### RENDER DEPLOYMENT
```
1. Go to Render.com
2. Sign up with GitHub
3. Click "New Web Service"
4. Select your GitHub repo
5. Configure:
   - Build Command: pip install -r requirements.txt
   - Start Command: streamlit run app.py --server.port=10000
6. Click "Create Web Service"
7. Wait 3-5 minutes for deployment
8. Get your live URL
```

### ENVIRONMENT VARIABLES (Important!)
```
STREAMLIT_SERVER_PORT = 10000
STREAMLIT_SERVER_HEADLESS = true
STREAMLIT_SERVER_ENABLECORS = false
```

### TESTING LIVE APP
```
1. Visit provided Render URL
2. Use sliders to input values
3. Click "Make Prediction"
4. Verify results display correctly
```

---

## üìã PART F: PROJECT SUBMISSION REQUIREMENTS

### FILES NEEDED FOR SUBMISSION
```
‚úì app.py (Streamlit app)
‚úì requirements.txt (dependencies)
‚úì model/model_building.ipynb (training code)
‚úì model/breast_cancer_model.pkl (trained model)
‚úì model/scaler.pkl (feature scaler)
‚úì BreastCancer_hosted_webGUI_link.txt (submission info)
```

### SUBMISSION FILE CONTENT (Must Include!)
```
Name: Ogah Victor
Matric Number: 22CG031902
Machine Learning Algorithm Used: Logistic Regression
Model Persistence Method: Joblib (.pkl files)
Live URL: https://your-render-app.onrender.com
GitHub Repository Link: https://github.com/username/repo
```

### DEADLINE: 11:59 PM January 22, 2026

---

## üîë KEY FORMULAS FOR EXAM

```
LOGISTIC REGRESSION
y = 1 / (1 + e^(-z))  where z = w‚ÇÄ + w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô

FEATURE SCALING
x_scaled = (x - mean) / std_dev

ACCURACY
Accuracy = (TP + TN) / (TP + TN + FP + FN)

PRECISION
Precision = TP / (TP + FP)

RECALL
Recall = TP / (TP + FN)

F1-SCORE
F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)
```

---

## ‚ùì EXAM STYLE QUESTIONS (Practice!)

### Q1: Why use StandardScaler?
**A:** To normalize features to same scale (mean=0, std=1), which improves Logistic Regression performance and prevents features with larger ranges from dominating the model.

### Q2: What's the difference between Precision and Recall?
**A:** Precision = accuracy of positive predictions. Recall = coverage of actual positives. In medical context, high recall is critical (don't miss cancer).

### Q3: What does 80-20 train-test split mean?
**A:** 80% of data used for training model, 20% used for unbiased evaluation. Prevents overfitting and ensures fair performance assessment.

### Q4: Why save the scaler with the model?
**A:** New input data must be scaled using the SAME scaler (same mean/std) that training data used. Different scaler = wrong predictions.

### Q5: What's the output of predict_proba()?
**A:** Array of probabilities for each class. Example: [0.03, 0.97] means 3% chance malignant, 97% chance benign.

### Q6: Why use Logistic Regression for this project?
**A:** Binary classification, simple, interpretable, fast training, ~96-97% accuracy, good for educational purposes.

### Q7: What does model.fit() do?
**A:** Trains the model on training data. Learns the relationship between features and target variable. Stores coefficients and intercept.

### Q8: What's the purpose of stratify=True in train_test_split()?
**A:** Maintains same class distribution in train and test sets. Important for imbalanced datasets to prevent biased evaluation.

---

## üéì IMPORTANT CONCEPTS FOR EXAM

### OVERFITTING
```
Problem: Model learns noise, poor generalization
Sign: High training accuracy, low test accuracy
Solution: Use test set, regularization, cross-validation
```

### BIAS-VARIANCE TRADEOFF
```
High Bias: Underfitting (too simple)
High Variance: Overfitting (too complex)
Goal: Balance both for best generalization
```

### CROSS-VALIDATION
```
Purpose: More robust evaluation
Method: Divide data into k folds, train k times
Benefit: Better estimate of real-world performance
```

### CLASS IMBALANCE
```
Problem: Different number of malignant vs benign cases
Solution: Use stratified split, check precision/recall
Why Important: Accuracy alone can be misleading
```

---

## ‚úÖ MUST-HAVE CODE SNIPPETS

### Load Model
```python
import joblib
model = joblib.load('model/breast_cancer_model.pkl')
scaler = joblib.load('model/scaler.pkl')
```

### Train Model
```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter=1000, solver='lbfgs')
model.fit(X_train_scaled, y_train)
```

### Make Prediction
```python
prediction = model.predict([[12, 18, 80, 500, 0.10]])  # [0 or 1]
probability = model.predict_proba([[12, 18, 80, 500, 0.10]])  # [P(0), P(1)]
```

### Evaluate Model
```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
accuracy = accuracy_score(y_test, predictions)
precision = precision_score(y_test, predictions)
recall = recall_score(y_test, predictions)
f1 = f1_score(y_test, predictions)
```

### Scale Features
```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # Transform only, not fit!
```

### Save Model
```python
joblib.dump(model, 'breast_cancer_model.pkl')
```

---

## üß† FINAL EXAM TIPS

1. **Understand WHY, not just HOW**: Understand why StandardScaler is used, why we split data, why evaluate on test set.

2. **Know the pipeline**: Data ‚Üí Preprocessing ‚Üí Training ‚Üí Evaluation ‚Üí Deployment

3. **Remember medical context**: In healthcare, false negatives (missing cancer) are worse than false positives (unnecessary tests).

4. **Key metrics**: Accuracy shows overall performance, but Precision/Recall are more important for medical diagnosis.

5. **Model persistence**: Models must be saved so they can be used without retraining.

6. **Streamlit basics**: Just input ‚Üí model ‚Üí output. Simple but powerful for demos.

7. **Deployment**: GitHub + Render = automated deployment with live URL.

8. **Review your code**: Go through model_building.ipynb step by step. Understand each cell.

---

## ‚è±Ô∏è QUICK 5-MINUTE REVIEW

```
What is the algorithm? ‚Üí Logistic Regression
What are the 5 features? ‚Üí radius, texture, perimeter, area, smoothness
How is accuracy? ‚Üí ~96-97%
What evaluation metrics? ‚Üí Accuracy, Precision, Recall, F1-Score
Why scale features? ‚Üí Same range, better performance
How save the model? ‚Üí Joblib
How deploy? ‚Üí GitHub ‚Üí Render
What's the output? ‚Üí Malignant or Benign with confidence
Is it medical tool? ‚Üí NO! Educational only!
```

---

**Good luck on your exam! You've got this! üìöüöÄ**

*Remember: Understanding is better than memorizing. Review the actual code in model_building.ipynb!*
